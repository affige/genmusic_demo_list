```A list of demo websites for automatic music generation research```

### interactive multi-track music composition
* [Jamming with Yating](http://mac.citi.sinica.edu.tw/~yang/pub/ailabs19ismirlbd_2.pdf) (RNN; hsiao19ismir-lbd): https://www.youtube.com/watch?v=9ZIJrr6lmHg

### interactive piano composition
* [Piano Genie](https://nips2018creativity.github.io/doc/pianogenie.pdf) (RNN; donahue18nips-creativity): https://piano-genie.glitch.me/
* [AI duet](https://nips.cc/Conferences/2016/Schedule?showEvent=6307) (RNN; roberts16nips-demo): https://experiments.withgoogle.com/ai/ai-duet/view/


### interactive monoaural music composition
* [musicalspeech] (Transformer; d'Eon20nips-demo): https://jasondeon.github.io/musicalSpeech/

### compose melody
* [meloform](https://arxiv.org/pdf/2208.14345.pdf) : https://ai-muzic.github.io/meloform/
* [parkR](https://transactions.ismir.net/articles/10.5334/tismir.87/) (markov; frieler22tismir): https://github.com/klausfrieler/parkR
* [xai-lsr](https://xai4debugging.github.io/files/papers/exploring_xai_for_the_arts_exp.pdf) (VAE; bryankinns21nipsw): https://xai-lsr-ui.vercel.app/
* [Trans-LSTM](https://archives.ismir.net/ismir2021/paper/000017.pdf) (Transformer+LSTM; dai21ismir): N/A...
* [diffusion](https://archives.ismir.net/ismir2021/paper/000058.pdf) (diffusion+musicVAE; mittal21ismir): https://storage.googleapis.com/magentadata/papers/symbolic-music-diffusion/index.html
* [MELONS](https://arxiv.org/pdf/2110.05020.pdf) (Transformer; zhou21arxiv): https://yiathena.github.io/MELONS/
* [Sketchnet](https://program.ismir2020.net/poster_1-09.html) (VAE+GRU; chen20ismir): https://github.com/RetroCirce/Music-SketchNet
* [SSMGAN](https://drive.google.com/file/d/1Ol4Ym3KqUkjcfL_Yeu0It3BP7NFS2mor/view) (VAE+LSTM+GAN; jhamtani19ml4md): https://drive.google.com/drive/folders/1TlOrbYAm7vGUvRrxa-uiH17bP-4N4e9z
* [StructureNet](http://ismir2018.ircam.fr/doc/pdfs/126_Paper.pdf) (LSTM; medeot18ismir) https://www.dropbox.com/sh/yxkxlnzi913ba50/AAA_mDbhdmaGJC9qj0zSlqCea?dl=0
* [MusicVAE](https://arxiv.org/abs/1803.05428) (LSTM+VAE; roberts18icml): https://magenta.tensorflow.org/music-vae
* [MidiNet](https://arxiv.org/abs/1703.10847) (CNN+GAN; yang17ismir): https://richardyang40148.github.io/TheBlog/midinet_arxiv_demo.html
* [C-RNN-GAN](https://mogren.one/publications/2016/c-rnn-gan/mogren2016crnngan.pdf) (LSTM+GAN; mogren16cml): http://mogren.one/publications/2016/c-rnn-gan/
* [folkRNN](https://github.com/IraKorshunova/folk-rnn) (LSTM): https://folkrnn.org/


### compose single-track polyphonic music
* [Compose & Embellish]() (Transformer; wu22arxiv): https://drive.google.com/drive/folders/1Y7HfExAz3PpPbFl0OnccxYDNF1KZUP-3
* [Theme Transformer](https://arxiv.org/abs/2111.04093) (Transformer; shih21arxiv): https://atosystem.github.io/ThemeTransformer/
* [EMOPIA](https://archives.ismir.net/ismir2021/paper/000039.pdf) (Transformer; hung21ismir): https://annahung31.github.io/EMOPIA/
* [dadagp](https://archives.ismir.net/ismir2021/paper/000076.pdf) (Transformer; sarmento21ismir): https://drive.google.com/drive/folders/1USNH8olG9uy6vodslM3iXInBT725zult
* [CP Transformer](https://arxiv.org/abs/2101.02402) (Transformer; hsiao21aaai): https://ailabs.tw/human-interaction/compound-word-transformer-generate-pop-piano-music-of-full-song-length/
* [PIANOTREE VAE](https://arxiv.org/abs/2008.07118) (VAE+GRU; wang20ismir): https://github.com/ZZWaang/PianoTree-VAE
* [Guitar Transformer](https://arxiv.org/abs/2008.01431) (Transformer; chen20ismir): https://ss12f32v.github.io/Guitar-Transformer-Demo/
* [Pop Music Transformer](https://arxiv.org/abs/2002.00212) (Transformer; huang20mm): https://github.com/YatingMusic/remi
* [Conditional Music Transformer](https://arxiv.org/abs/1912.05537) (Transformer; choi19arxiv): https://storage.googleapis.com/magentadata/papers/music-transformer-autoencoder/index.html; and https://magenta.tensorflow.org/transformer-autoencoder
* [PopRNN](http://mac.citi.sinica.edu.tw/~yang/pub/ailabs19ismirlbd_1.pdf) (RNN; yeh19ismir-lbd): https://soundcloud.com/yating_ai/sets/ismir-2019-submission/
* [VGMIDI](http://www.lucasnferreira.com/papers/2019/ismir-learning.pdf) (LSTM; ferreira19ismir): https://github.com/lucasnfe/music-sentneuron
* [Amadeus](https://arxiv.org/pdf/1902.01973.pdf) (LSTM+RL; kumar19arxiv): https://goo.gl/ogVMSq
* [Modularized VAE](https://arxiv.org/pdf/1811.00162.pdf) (GRU+VAE; wang19icassp): https://github.com/MiuLab/MVAE_Music
* [BachProp](https://arxiv.org/abs/1812.06669) (GRU; colombo18arxiv): https://sites.google.com/view/bachprop
* [Music Transformer](https://arxiv.org/abs/1809.04281) (Transformer; huang19iclr): https://magenta.tensorflow.org/music-transformer

### compose single-track polyphonic music by combinging existing ones
* [CollageNet](https://archives.ismir.net/ismir2021/paper/000098.pdf) (VAE; wuerkaixi21ismir): https://github.com/urkax/CollageNet

### compose multi-track music
* [Museformer](https://arxiv.org/abs/2210.10349) (Transformer; yu22neurips): https://ai-muzic.github.io/museformer/
* [SymphonyNet](https://arxiv.org/pdf/2205.05448.pdf) (Transformer; liu22ismir): https://symphonynet.github.io/
* [CMT](https://arxiv.org/abs/2111.08380) (Transformer; di21mm): https://wzk1015.github.io/cmt/
* [CONLON](https://program.ismir2020.net/poster_6-14.html) (GAN; angioloni20ismir): https://paolo-f.github.io/CONLON/
* [MMM](https://arxiv.org/pdf/2008.06048.pdf) (Transformer; ens20arxiv): https://jeffreyjohnens.github.io/MMM/
* [MahlerNet](http://www.mahlernet.se/files/SMC2019.pdf) (RNN+VAE; lousseief19smc): https://github.com/fast-reflexes/MahlerNet
* [measure-by-measure](https://openreview.net/forum?id=Hklk6xrYPB) (RNN): https://sites.google.com/view/pjgbjzom
* [JazzRNN](http://mac.citi.sinica.edu.tw/~yang/pub/ailabs19ismirlbd_1.pdf) (RNN; yeh19ismir-lbd): https://soundcloud.com/yating_ai/sets/ismir-2019-submission/
* [MIDI-Sandwich2](https://arxiv.org/pdf/1909.03522.pdf) (RNN+VAE; liang19arxiv): https://github.com/LiangHsia/MIDI-S2
* [LakhNES](https://arxiv.org/abs/1907.04868) (Transformer; donahue19ismir): https://chrisdonahue.com/LakhNES/
* [MuseNet](https://openai.com/blog/musenet/) (Transformer): https://openai.com/blog/musenet/
* [MIDI-VAE](https://arxiv.org/abs/1809.07600) (GRU+VAE; brunner18ismir): https://www.youtube.com/channel/UCCkFzSvCae8ySmKCCWM5Mpg
* [Multitrack MusicVAE](https://arxiv.org/abs/1806.00195) (LSTM+VAE; simon18ismir): https://magenta.tensorflow.org/multitrack
* [MuseGAN](https://arxiv.org/abs/1709.06298) (CNN+GAN; dong18aaai): https://salu133445.github.io/musegan/

### compose multitrack music (cover generation; need reference MIDI)
* [FIGARO](https://arxiv.org/abs/2201.10936) (Transformer; r√ºtte22arxiv): https://github.com/dvruette/figaro

### given chord, compose melody
* [H-EC2-VAE](https://archives.ismir.net/ismir2021/paper/000092.pdf) (GRU+VAE; wei21ismir): N/A...
* [MINGUS](https://archives.ismir.net/ismir2021/paper/000051.pdf) (Transformer; madaghiele21ismir): https://github.com/vincenzomadaghiele/MINGUS
* [BebopNet](https://program.ismir2020.net/poster_6-08.html) (LSTM): https://shunithaviv.github.io/bebopnet/
* [JazzGAN](http://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf) (GAN; trieu18mume): https://www.cs.hmc.edu/~keller/jazz/improvisor/
* [XiaoIce Band](http://staff.ustc.edu.cn/~qiliuql/files/Publications/Hongyuan-Zhu-KDD2018.pdf) (GRU; zhu18kdd): http://tv.cctv.com/2017/11/24/VIDEo7JWp0u0oWRmPbM4uCBt171124.shtml

### given melody, compose chord (harmonization)
* [DAT-CVAE](https://arxiv.org/pdf/2209.07144.pdf) (transformer-vae; zhao22ismir): https://zhaojw1998.github.io/DAT_CVAE
* [SurpriseNet](https://arxiv.org/pdf/2108.00378.pdf) (VAE; chen21ismir): https://github.com/scmvp301135/SurpriseNet
* [MTHarmonizer](https://arxiv.org/pdf/2001.02360.pdf) (RNN; yeh21jnmr)


### given lyrics, compose melody
* [ROC](https://arxiv.org/pdf/2208.05697.pdf) 
* [Conditional LSTM-GAN](https://arxiv.org/pdf/1908.05551.pdf) (LSTM+GAN; yu19arxiv): https://github.com/yy1lab/Lyrics-Conditioned-Neural-Melody-Generation
* [iComposer](https://www.aclweb.org/anthology/N19-4015) (LSTM; lee19acl): https://www.youtube.com/watch?v=Gstzqls2f4A
* [SongWriter](https://arxiv.org/pdf/1809.04318.pdf) (GRU; bao18arxiv): N/A


### compose drum MIDI
* [Nuttall's model](https://nime.pubpub.org/pub/8947fhly/release/1?readingCollection=71dd0131) (Transformer; nuttall21nime): https://nime.pubpub.org/pub/8947fhly/release/1?readingCollection=71dd0131
* [Wei's model](https://drive.google.com/file/d/1149HnGliYtl45Cjp9XwJadL_YHRLvq5F/view) (VAE+GAN; wei19ismir): https://github.com/Sma1033/drum_generation_with_ssm
* [DrumNet](https://arxiv.org/pdf/1908.00948.pdf) (GAE; lattner19waspaa): https://sites.google.com/view/drum-generation
* [DrumVAE](https://arxiv.org/abs/1902.03722) (GRU+VAE; thio19milc): http://vibertthio.com/drum-vae-client


### compose melody+chords (two tracks)
* [Jazz Transformer](https://arxiv.org/abs/2008.01307) (Transformer; wu20ismir): https://drive.google.com/drive/folders/1-09SoxumYPdYetsUWHIHSugK99E2tNYD
* [Transformer VAE](https://ieeexplore.ieee.org/document/9054554) (Transformer+VAE; jiang20icassp): https://drive.google.com/drive/folders/1Su-8qrK__28mAesSCJdjo6QZf9zEgIx6
* [Two-stage RNN](https://arxiv.org/abs/2002.10266) (RNN; deboom20arxiv): https://users.ugent.be/~cdboom/music/
* [LeadsheetGAN](https://arxiv.org/abs/1807.11161) (CRNN+GAN; liu18icmla): https://liuhaumin.github.io/LeadsheetArrangement/results
* [LeadsheetVAE](https://drive.google.com/file/d/10uGRGEI9IOfu_LyzDSG393fGhwUrEOi4/view) (RNN+VAE; liu18ismir-lbd): https://liuhaumin.github.io/LeadsheetArrangement/results


### given melody, compose arrangement
* [SongDriver](https://arxiv.org/pdf/2209.06054.pdf) (Transformer-CRF; wang22mm): 
* [AccoMontage2](https://arxiv.org/pdf/2209.00353.pdf) : https://billyyi.top/accomontage2/
* [AccoMontage](https://archives.ismir.net/ismir2021/paper/000104.pdf) (template-based; zhao21ismir): https://github.com/zhaojw1998/AccoMontage
* [CP Transformer](https://arxiv.org/abs/2101.02402) (Transformer; hsiao21aaai): https://ailabs.tw/human-interaction/compound-word-transformer-generate-pop-piano-music-of-full-song-length/
* [PopMAG](https://arxiv.org/abs/2008.07703) (transformer; ren20mm): https://music-popmag.github.io/popmag/
* LeadsheetGAN: see above
* LeadsheetVAE: see above
* XiaoIce Band (the "multi-instrument co-arrangement model"): N/A


### given mix (audio), compose bass
* [BassNet](https://www.mdpi.com/2076-3417/10/18/6627) (GAE+CNN; ren20mm): https://sonycslparis.github.io/bassnet/


### given prime melody, compose melody+chords
* [local_conv_music_generation](http://ouyangzhihao.com/wp-content/uploads/2018/12/MUSIC-GENERATION-WITH-LOCAL-CONNECTED-CONVOLUTIONAL-NEURAL-NETWORK.pdf) (CNN; ouyang18arxiv): https://somedaywilldo.github.io/local_conv_music_generation/


### given prime melody, compose melody+chords+bass
* [BandNet](https://arxiv.org/abs/1812.07126) (RNN; zhou18arxiv): https://soundcloud.com/yichao-zhou-555747812/sets/bandnet-sound-samples-1 


### given piano score, compose an orchestration
* [LOP](https://qsdfo.github.io/LOP/index.html) (RBM; crestel17smc): https://qsdfo.github.io/LOP/results.html

### piano infilling
* [structure-aware infilling](https://arxiv.org/pdf/2210.02829.pdf) : https://tanchihpin0517.github.io/structure-aware_infilling
* [VLI](https://arxiv.org/pdf/2108.05064.pdf) (Transformer; chang21ismir): https://jackyhsiung.github.io/piano-infilling-demo/
* [The Piano Inpainting Application](https://arxiv.org/pdf/2107.05944.pdf) (): https://ghadjeres.github.io/piano-inpainting-application/

### melody infilling
* [CLSM](https://archives.ismir.net/ismir2021/paper/000002.pdf) (Transformer+LSTM; akama21ismir): https://contextual-latent-space-model.github.io/demo/

### symbolic-domain genre style transfer
* [Pop2Jazz](http://mac.citi.sinica.edu.tw/~yang/pub/ailabs19ismirlbd_1.pdf) (RNN; yeh19ismir-lbd): https://soundcloud.com/yating_ai/sets/ismir-2019-submission/
* [Groove2Groove](https://hal.archives-ouvertes.fr/hal-02923548/document) (RNN; c√≠fka19ismir, c√≠fka20taslp): https://groove2groove.telecom-paris.fr/
* [CycleGAN2](https://tik-old.ee.ethz.ch/file/0d41d7d657f1a65f65373c4797caaeac/Music_Genre_Transfer___ECML_MML_Workshop_CR.pdf) (CNN+GAN; brunner19mml): https://drive.google.com/drive/folders/1Jr_p6pnKvhA2YW9sp-ABChiFgV3gY1aT
* [CycleGAN](https://arxiv.org/pdf/1809.07575.pdf) (CNN+GAN; brunner18ictai): https://github.com/sumuzhao/CycleGAN-Music-Style-Transfer
* [FusionGAN](https://dac.cs.vt.edu/wp-content/uploads/2017/11/learning-to-fuse.pdf) (GAN; chen17icdm): http://people.cs.vt.edu/czq/publication/fusiongan/


### symbolic-domain arrangement style transfer
* [UnetED](https://arxiv.org/abs/1905.13567) (CNN+Unet; hung19ijcai): https://biboamy.github.io/disentangle_demo/result/index.html


### symbolic-domain emotion/rhythm/pitch style transfer
* [MuseMorphose](https://arxiv.org/abs/2105.04090) (Transformer+VAE; wu21arxiv): https://slseanwu.github.io/site-musemorphose/
* [Kawai](https://program.ismir2020.net/poster_5-06.html) (VAE+GRU+adversarial; kawai20ismir): https://lisakawai.github.io/music_transformation/
* [Wang](https://program.ismir2020.net/poster_5-05.html) (VAE+GRU; wang20ismir): https://github.com/ZZWaang/polyphonic-chord-texture-disentanglement
* [Music FaderNets](https://program.ismir2020.net/poster_1-13.html) (VAE; tan20ismir): https://music-fadernets.github.io/
* [deep-music-analogy](https://arxiv.org/pdf/1906.03626.pdf) (yang19ismir): https://github.com/cdyrhjohn/Deep-Music-Analogy-Demos


### given MIDI, generate human-like MIDI: Piano only
* [CVRNN](http://archives.ismir.net/ismir2019/paper/000105.pdf) (CVRNN; maezawa19ismir): https://sites.google.com/view/cvrnn-performance-render
* [GGNN](http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf) (graph NN + hierarchical attention RNN; jeong19icml)
* [VirtuosoNet](https://nips2018creativity.github.io/doc/virtuosonet.pdf) (LSTM+hierarchical attention network; jeong18nipsw): https://www.youtube.com/playlist?list=PLkIVXCxCZ08rD1PXbrb0KNOSYVh5Pvg-c
* [PerformanceRNN](https://magenta.tensorflow.org/performance-rnn) (RNN): https://magenta.tensorflow.org/performance-rnn


### given MIDI, generate human-like MIDI: Drum only
* [GrooVAE](https://magenta.tensorflow.org/groovae) (seq2seq+VAE; gillick19icml): https://magenta.tensorflow.org/groovae



### given score, generate musical audio (performance): Piano only
* [Wave2Midi2Wave](https://arxiv.org/abs/1810.12247) (transformer+wavenet; hawthorne19iclr): https://magenta.tensorflow.org/maestro-wave2midi2wave


### given score, generate musical audio (performance): Not limited to Piano
* [Deep Performer](https://arxiv.org/pdf/2202.06034.pdf) (Transformer; dong22icassp): https://salu133445.github.io/deepperformer/
* [PerformanceNet](https://arxiv.org/abs/1811.04357) (CNN+GAN; wang19aaai): https://github.com/bwang514/PerformanceNet
* [Conditioned Wavenet](https://archives.ismir.net/ismir2018/paper/000192.pdf) (Wavenet; manzelli18ismir): http://people.bu.edu/bkulis/projects/music/index.html


### audio/timbre synthesis
* [GANStrument](https://arxiv.org/abs/2211.05385) (narita22arxiv): https://ganstrument.github.io/ganstrument-demo/
* [NEWT](https://archives.ismir.net/ismir2021/paper/000031.pdf) (DDSP; hayes21ismir): https://benhayes.net/projects/nws/
* [CRASH](https://archives.ismir.net/ismir2021/paper/000072.pdf) (diffusion; rouard21ismir): https://crash-diffusion.github.io/crash/
* [DarkGAN](https://archives.ismir.net/ismir2021/paper/000060.pdf) (GAN; nistal21ismir): https://an-1673.github.io/DarkGAN.io/
* [MP3net](https://arxiv.org/abs/2101.04785) (GAN; broek21arxiv): https://korneelvdbroek.github.io/mp3net/
* [Michelashvili](https://program.ismir2020.net/poster_6-19.html) (dsp-inspired; michelashvili20iclr): https://github.com/mosheman5/timbre_painting
* [GAAE](https://arxiv.org/abs/2006.00877) (GAN+AAE; haque20arxiv): https://drive.google.com/drive/folders/1et_BuZ_XDMrdsYzZDprLvEpmmuZrJ7jk
* [MANNe](https://arxiv.org/abs/2001.11296) (): https://github.com/JTColonel/manne
* [DDSP](https://openreview.net/forum?id=B1x1ma4tDr) (dsp-inspired; lamtharn20iclr): https://storage.googleapis.com/ddsp/index.html
* [MelNet](https://arxiv.org/pdf/1906.01083.pdf) (auto-regressive; vasquez19arxiv): https://audio-samples.github.io/
* [AdVoc](https://arxiv.org/abs/1904.07944) (; neekhara19arxiv): http://chrisdonahue.com/advoc_examples/
* [GANSynth](https://arxiv.org/abs/1902.08710) (CNN+GAN; engel19iclr): https://magenta.tensorflow.org/gansynth
* [SynthNet](https://www.ijcai.org/proceedings/2019/467) (schimbinschi19ijcai): https://www.dropbox.com/sh/hkp3o5xjyexp2x0/AADvrfXTbHBXs9W7GN6Yeorua?dl=0
* [TiFGAN](https://arxiv.org/abs/1902.04072) (CNN+GAN; marafioti19arxiv): https://tifgan.github.io/
* [SING](https://arxiv.org/abs/1810.09785) (defossez18nips): https://research.fb.com/wp-content/themes/fb-research/research/sing-paper/
* [WaveGAN](https://arxiv.org/abs/1802.04208) (CNN+GAN; donahue19iclr): https://github.com/chrisdonahue/wavegan
* [NSynth](https://arxiv.org/abs/1704.01279) (WaveNet; engel17arxiv): https://magenta.tensorflow.org/nsynth


### audio-domain music generation 
* [musika!](https://arxiv.org/pdf/2208.08706.pdf) (GAN; pasini22ismir): https://huggingface.co/spaces/marcop/musika
* [jukebox](https://arxiv.org/abs/2005.00341) (VQVAE+Transformer; dhariwal20arxiv): https://openai.com/blog/jukebox/
* [UNAGAN](https://arxiv.org/abs/2005.08526) (GAN; liu20arxiv): https://github.com/ciaua/unagan
* [dadabots](https://arxiv.org/abs/1811.06633) (sampleRNN; carr18mume): http://dadabots.com/music.php

### text-to-music
* [MusicLM](https://arxiv.org/abs/2301.11325) (Transformer; agostinelli23arxiv): https://google-research.github.io/seanet/musiclm/examples/
* [multi-source-diffusion-models](https://arxiv.org/abs/2302.02257) (diffusion; 23arxiv): https://gladia-research-group.github.io/multi-source-diffusion-models/
* [Noise2Music](https://arxiv.org/abs/2302.03917) (diffusion; huang23arxiv): https://noise2music.github.io/
* [ERNIE-Music](https://arxiv.org/pdf/2302.04456.pdf) (diffusion; zhu23arxiv): N/A
* [riffusion]() (diffusion;): https://www.riffusion.com/
* [Make-An-Audio](https://text-to-audio.github.io/paper.pdf) (diffusion; huang23arxiv): https://text-to-audio.github.io/ (for general sounds)
* [AudioLDM](https://arxiv.org/pdf/2301.12503.pdf) (diffusion; liu23arxiv): https://audioldm.github.io/ (for general sounds)
* [AudioLM](https://arxiv.org/abs/2209.03143) (Transformer; borsos22arxiv): https://google-research.github.io/seanet/audiolm/examples/ (for general sounds)


### given singing, generate accompaniments
* [singsong](https://arxiv.org/abs/2301.12662) (VQVAE+Transofmrer; donahue23arxiv): https://storage.googleapis.com/sing-song/index.html


### given drumless audio, generate drum accompaniments
* [jukedrummer](https://arxiv.org/pdf/2210.06007.pdf) (VQVAE+Transofmrer; wu22ismir): https://legoodmanner.github.io/jukedrummer-demo/


### audio-domain singing synthesis
* [NANSY++](https://arxiv.org/abs/2211.09407) (Transformer; choi23iclr): https://bald-lifeboat-9af.notion.site/Demo-Page-For-NANSY-67d92406f62b4630906282117c7f0c39
* [VISinger 2](https://arxiv.org/abs/2211.02903) (zhang22arxiv): https://zhangyongmao.github.io/VISinger2/
* [xiaoicesing 2](https://arxiv.org/abs/2210.14666) (Transformer+GAN; wang22arxiv): https://wavelandspeech.github.io/xiaoice2/
* [WeSinger 2](https://arxiv.org/pdf/2207.01886.pdf) (Transformer+GAN; zhang22arxiv): https://zzw922cn.github.io/wesinger2/
* [U-Singer](https://arxiv.org/pdf/2203.00931.pdf) (Transformer; kim22arxiv): https://u-singer.github.io/
* [Singing-Tacotron](https://arxiv.org/pdf/2202.07907.pdf) (Transformer; wang22arxiv): https://hairuo55.github.io/SingingTacotron/
* [KaraSinger](https://arxiv.org/abs/2110.04005) (GRU/Transformer; liao22icassp): https://jerrygood0703.github.io/KaraSinger/
* [VISinger](https://arxiv.org/abs/2110.08813) (flow; zhang2): https://zhangyongmao.github.io/VISinger/
* [MLP singer](https://arxiv.org/abs/2106.07886) (mixer blocks; tae21arxiv): https://github.com/neosapience/mlp-singer
* [LiteSing](https://ieeexplore.ieee.org/document/9414043) (wavenet; zhuang21icassp): https://auzxb.github.io/LiteSing/
* [DiffSinger](https://arxiv.org/abs/2105.02446) (diffusion; liu22aaai)[no duration modeling]: https://diffsinger.github.io/
* [HiFiSinger](https://arxiv.org/abs/2009.01776) (Transformer; chen20arxiv): https://speechresearch.github.io/hifisinger/
* [DeepSinger](https://arxiv.org/abs/2007.04590) (Transformer; ren20kdd): https://speechresearch.github.io/deepsinger/
* [xiaoice-multi-singer](https://arxiv.org/pdf/2006.10317.pdf): https://jiewu-demo.github.io/INTERSPEECH2020/
* [xiaoicesing](https://arxiv.org/pdf/2006.06261.pdf): https://xiaoicesing.github.io/
* [bytesing](https://arxiv.org/pdf/2004.11012.pdf): https://bytesings.github.io/ 
* [mellotron](https://arxiv.org/abs/1910.11997): https://nv-adlr.github.io/Mellotron
* [lee's model](https://arxiv.org/pdf/1908.01919.pdf) (lee19arxiv): http://ksinging.mystrikingly.com/
* http://home.ustc.edu.cn/~yiyh/interspeech2019/


### audio-domain singing style transfer
* [Assem-VC](https://neuripscreativityworkshop.github.io/2021/accepted/ncw_58.pdf) (; kim21nipsw): https://mindslab-ai.github.io/assem-vc/singer/
* [iZotope-SVC](https://program.ismir2020.net/poster_1-08.html) (conv-encoder/decoder; nercessian20ismir): https://sites.google.com/izotope.com/ismir2020-audio-demo
* [VAW-GAN](https://arxiv.org/pdf/2008.03992.pdf) (GAN; lu20arxiv): https://kunzhou9646.github.io/singvaw-gan/
* [polyak20interspeech](https://arxiv.org/pdf/2008.02830.pdf) (GAN; polyak20interspeech): https://singing-conversion.github.io/
* [speech2sing2](https://arxiv.org/pdf/2005.13835.pdf) (GAN; wu20interspeech): https://ericwudayi.github.io/Speech2Singing-DEMO/
* [speech2sing](https://arxiv.org/pdf/2002.06595.pdf) (encoder/decoder; parekh20icassp): https://jayneelparekh.github.io/icassp20/
* [SINGAN](https://www.researchgate.net/publication/336058156_SINGAN_Singing_Voice_Conversion_with_Generative_Adversarial_Networks) (GAN; sisman19apsipa): N/A
* [MSVC-GAN] (GAN): https://hujinsen.github.io/
* https://mtg.github.io/singing-synthesis-demos/voice-cloning/
* https://enk100.github.io/Unsupervised_Singing_Voice_Conversion/
* [Yong&Nam](https://seyong92.github.io/publications/yong_ICASSP_2018.pdf) (DSP; yong18icassp): https://seyong92.github.io/singing-expression-transfer/
* [cybegan](https://arxiv.org/pdf/1807.02254.pdf) (CNN+GAN; wu18faim): http://mirlab.org/users/haley.wu/cybegan/


### audio-domain singing correction
* [deep-autotuner](https://arxiv.org/abs/1902.00956) (CGRU; wagner19icassp): http://homes.sice.indiana.edu/scwager/deepautotuner.html


### audio-domain style transfer (general)
* [VQ-VAE](https://arxiv.org/pdf/2102.05749.pdf) (VQ-VAE; cifka21icassp): https://adasp.telecom-paris.fr/rc/demos_companion-pages/cifka-ss-vq-vae/
* [MelGAN-VC](https://arxiv.org/pdf/1910.03713.pdf) (GAN; pasini19arxiv): https://www.youtube.com/watch?v=3BN577LK62Y&feature=youtu.be
* [RaGAN](https://www.aaai.org/Papers/AAAI/2019/AAAI-LuC.2259.pdf) (GAN; lu19aaai): https://github.com/ChienYuLu/Play-As-You-Like-Timbre-Enhanced-Multi-modal-Music-Style-Transfer
* [TimbreTron](http://www.cs.toronto.edu/~huang/TimbreTron/pdf/TImbreTron_arxiv.pdf) (GAN; huang19iclr): https://www.cs.toronto.edu/~huang/TimbreTron/samples_page.html
* [string2woodwind](https://minjekim.com/papers/icassp2017_swager.pdf) (DSP; wagner17icassp): http://homes.sice.indiana.edu/scwager/css.html

### TTS
* [VITS](https://arxiv.org/abs/2106.06103) (transformer+flow+GAN; kim21icml): https://github.com/jaywalnut310/vits

### vocoder
* [sawsing](https://arxiv.org/pdf/2208.04756.pdf) (DDSP; wu22ismir): https://ddspvocoder.github.io/ismir-demo/
* [Multi-Singer](https://dl.acm.org/doi/abs/10.1145/3474085.3475437) (wavenet; huang21mm): https://multi-singer.github.io/
* [SingGAN](https://arxiv.org/pdf/2110.07468.pdf) (GAN; chen21arxiv): https://singgan.github.io/
* [DiffWave](https://arxiv.org/abs/2009.09761) (diffusion; kong21iclr): https://diffwave-demo.github.io/
* [MelGAN](https://arxiv.org/abs/1910.06711) (GAN; kumar19neurips): https://melgan-neurips.github.io/

### audio-domain loop generation
* [pjloopgan](https://arxiv.org/pdf/2209.01751.pdf) (GAN; yeh22ismir): https://arthurddd.github.io/PjLoopGAN/
* [loopgen](https://archives.ismir.net/ismir2021/paper/000038.pdf) (GAN; hung21ismir): https://loopgen.github.io/
